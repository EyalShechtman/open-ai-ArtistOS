{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe5c97f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.13.3)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/nadyashechtman/Documents/Projects/open-ai-ArtistOS/music_mixing/venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os, numpy as np, torch\n",
    "from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor\n",
    "from qwen_omni_utils import process_mm_info\n",
    "import time\n",
    "\n",
    "# --- Apple Silicon-safe env (no CUDA/vLLM/flash-attn) ---\n",
    "os.environ.pop(\"CUDA_VISIBLE_DEVICES\", None)\n",
    "os.environ.pop(\"VLLM_USE_V1\", None)\n",
    "os.environ.pop(\"VLLM_WORKER_MULTIPROC_METHOD\", None)\n",
    "\n",
    "MODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Instruct\"   # << audio captioner = audio->text\n",
    "USE_AUDIO_IN_VIDEO = False                          # audio-only\n",
    "RETURN_AUDIO = False                                # we just want text\n",
    "\n",
    "# Prefer MPS if available, else CPU\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "print(f'time:{time.time()} processor')\n",
    "processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)\n",
    "\n",
    "print(f'time:{time.time()} loading model')\n",
    "# NOTE: flash_attention_2 is NOT supported on MPS, so omit it\n",
    "model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    dtype=\"auto\",            # default dtype; MPS often runs fp16 internally\n",
    "    device_map={\"\": device}, # send everything to MPS/CPU\n",
    ")\n",
    "\n",
    "def transcribe_audio(audio_path_or_url: str, prompt: str = \"Describe the style, rhythm, dynamics, and expressed emotions of this piece of music. Identify the instruments used and suggest possible scenarios from which this music might originate.\"):\n",
    "    # Build messages: audio + task text\n",
    "    print(f'time:{time.time()} build messages')\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"audio\", \"audio\": audio_path_or_url},\n",
    "            {\"type\": \"text\",  \"text\": prompt},\n",
    "        ],\n",
    "    }]\n",
    "\n",
    "    print(f'time:{time.time()} text apply chat template')\n",
    "    text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    print(f'time:{time.time()} process_mm_info')\n",
    "    audios, images, videos = process_mm_info(messages, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "\n",
    "    print(f'time:{time.time()} inputs')\n",
    "    inputs = processor(\n",
    "        text=text,\n",
    "        audio=audios,\n",
    "        images=images,\n",
    "        videos=videos,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        use_audio_in_video=USE_AUDIO_IN_VIDEO\n",
    "    )\n",
    "\n",
    "    # Move to device & dtype\n",
    "    inputs = {k: (v.to(device) if hasattr(v, \"to\") else v) for k, v in inputs.items()}\n",
    "    if hasattr(model, \"dtype\"):\n",
    "        for k in (\"input_ids\", \"attention_mask\"):\n",
    "            if k in inputs and hasattr(inputs[k], \"to\"):\n",
    "                inputs[k] = inputs[k].to(model.dtype)\n",
    "\n",
    "    print(f'time:{time.time()} generate')\n",
    "    # Generate text (no audio out)\n",
    "    text_ids, _ = model.generate(\n",
    "        **inputs,\n",
    "        thinker_return_dict_in_generate=True,\n",
    "        thinker_max_new_tokens=2048,\n",
    "        thinker_do_sample=False,\n",
    "        return_audio=False,\n",
    "        use_audio_in_video=USE_AUDIO_IN_VIDEO,\n",
    "    )\n",
    "\n",
    "    print(f'time:{time.time()} batch_decode')\n",
    "    out = processor.batch_decode(\n",
    "        text_ids.sequences[:, inputs[\"input_ids\"].shape[1]:],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    print(f'time:{time.time()} out')\n",
    "    return out.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa9550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
